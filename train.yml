dataset:
  path: ./data
  type: text
  p_train: 0.8
  parameters:
    keep_linebreaks: True

tokenizer:
  type: gpt2

model:
  type: distilgpt2

training_args:
  output_dir: /outputs/models
  load_best_model_at_end: True
  num_train_epochs: 10
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  evaluation_strategy: epoch
  logging_strategy: epoch
  save_strategy: epoch
  warmup_ratio: 0.2
  learning_rate: 0.000001

input:
  path: /inputs/data
  pattern: "*.txt"
  sep: "\n"

dataset:
  p_train: 0.8

tokenizer:
  type: gpt2
  parameters:
    use_fast: False

encoding_args: {}

model:
  type: distilgpt2
  parameters: {}

training_args:
  output_dir: /outputs/model
  num_train_epochs: 10
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  evaluation_strategy: steps
  logging_steps: 50
  save_steps: 50
  warmup_ratio: 0.2
  weight_decay: 0.01
  learning_rate: 0.0001

input:
  path: /inputs/data
  pattern: "*.txt"
  sep: "\n"

dataset:
  p_train: 0.8

tokenizer:
  type: gpt2
  parameters:
    use_fast: False

encoding_args: {}

model:
  type: distilgpt2
  parameters: {}

training_args:
  output_dir: /outputs/model
  load_best_model_at_end: True
  num_train_epochs: 30
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  evaluation_strategy: epoch
  logging_strategy: epoch
  save_strategy: epoch
  warmup_ratio: 0.2
  learning_rate: 0.001
